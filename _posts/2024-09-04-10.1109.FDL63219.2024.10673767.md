---
layout: home
tags: [News, Publications]
title: "New Conference Paper Published"
description: A new conference paper co-authored by Enrico Fraccaroli has been published in the proceedings of the 2024 Forum on Specification & Design Languages (FDL).
author: Enrico Fraccaroli
background: /assets/theme/images/publication_conference.png
comments: true
---

We are pleased to announce the publication of our latest conference paper,
titled **"Enhancing Split Computing and Early Exit Applications through
Predefined Sparsity"**, in the proceedings of the **2024 Forum on Specification
& Design Languages (FDL)**.

### Abstract

In the past decade, Deep Neural Networks (DNNs) achieved state-of-the-art
performance in a broad range of problems, spanning from object classification
and action recognition to smart building and healthcare. The flexibility that
makes DNNs such a pervasive technology comes at a price: the computational
requirements preclude their deployment on most of the resource-constrained edge
devices available today to solve real-time and real-world tasks. This paper
introduces a novel approach to address this challenge by combining the concept
of predefined sparsity with Split Computing (SC) and Early Exit (EE). In
particular, SC aims at splitting a DNN with a part of it deployed on an edge
device and the rest on a remote server. Instead, EE allows the system to stop
using the remote server and rely solely on the edge device’s computation if the
answer is already good enough. Specifically, how to apply such a predefined
sparsity to a SC and EE paradigm has never been studied. This paper studies this
problem and shows how predefined sparsity significantly reduces the
computational, storage, and energy burdens during the training and inference
phases, regardless of the hardware platform. This makes it a valuable approach
for enhancing the performance of SC and EE applications. Experimental results
showcase reductions exceeding 4× in storage and computational complexity without
compromising performance. The source code is available at
[GitHub](https://github.com/intelligolabs/sparsity_sc_ee).

### Details

- **Authors**: Luigi Capogrosso, Enrico Fraccaroli, Giulio Petrozziello, Francesco Setti, Samarjit Chakraborty, Franco Fummi, Marco Cristani
- **Conference**: Forum on Specification & Design Languages (FDL)
- **Year**: 2024
- **Pages**: 1-8
- **Keywords**: Performance evaluation, Training, Smart buildings, Source coding, Artificial neural networks, Medical services, Real-time systems, Split Computing, Early Exit, Deep Neural Networks, Predefined Sparsity, Edge Devices

### Links

- **DOI**: [10.1109/FDL63219.2024.10673767](https://doi.org/10.1109/FDL63219.2024.10673767)  
- **Open Access Version**: [Read Here](https://iris.univr.it/retrieve/3a876695-380e-4612-8e4a-01fbf1d38572/OPEN__2024__FDL__Enhancing_Split_Computing_and_Early_Exit_Applications_through_Predefined_Sparsity.pdf)  
- **Source Code**: [GitHub Repository](https://github.com/intelligolabs/sparsity_sc_ee)

This paper presents a novel method for improving the performance of Split
Computing (SC) and Early Exit (EE) applications by applying predefined sparsity.
The approach significantly reduces computational, storage, and energy demands
during both training and inference phases, making it an effective solution for
deploying DNNs on resource-constrained edge devices. Experimental results show
more than a 4× reduction in storage and computational complexity without
performance loss.

We extend our gratitude to all collaborators and contributors for their efforts.
For further details, please explore the links provided above.
